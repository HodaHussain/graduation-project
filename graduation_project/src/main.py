
# import re
# from typing import Tuple
# import whisper
# import os
# import tempfile
# import sounddevice as sd
# import wave
# from nltk.tokenize import word_tokenize
# from autocorrect import Speller
# from spellchecker import SpellChecker
# from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
# import nltk
# # âœ… Define Expanded Command Lists
# actions_list_en = [
#     "turn on", "turn off", "switch on", "switch off", "activate", "deactivate",
#     "open", "close", "lock", "unlock", "increase", "decrease", "raise", "lower",
#     "set", "adjust", "change", "dim", "brighten", "make brighter", "make dimmer",
#     "change color to", "set brightness to", "set fan speed to", "start", "stop",
#     "pause", "resume", "schedule", "set timer for", "turn on at", "turn off at",
#     "enable", "disable", "sync", "connect", "show status", "check status",
#     "is it on", "is it off", "good morning", "good night", "movie mode", "night mode"
# ]

# devices_list_en = [
#     "lights", "light", "leds", "lamp", "bulb", "ceiling light", "strip lights",
#     "fan", "ceiling fan", "exhaust fan", "desk fan", "door", "front door",
#     "main door", "back door", "garage door", "camera", "security camera", "cctv",
#     "surveillance", "curtains", "blinds", "shades", "window covers"
# ]

# rooms_list_en = [
#     "living room", "hall", "lounge", "main room", "reception", "bedroom",
#     "master bedroom", "guest room", "my room", "kitchen", "cooking area",
#     "dining area", "bathroom", "restroom", "toilet", "washroom", "balcony",
#     "terrace", "patio", "porch", "garage", "carport"
# ]

# actions_list_ar = [
#     "Ø´ØºÙ„", "Ø£Ø´ØºÙ„", "Ø§ÙØªØ­", "Ø£ÙØªØ­", "Ø´ØºÙ‘Ù„", "Ù‚Ù… Ø¨ØªØ´ØºÙŠÙ„", "Ù‚Ù… Ø¨Ø¥Ø¶Ø§Ø¡Ø©", "Ø£Ø·ÙØ¦",
#     "Ø§ØºÙ„Ù‚", "Ø¥ÙŠÙ‚Ø§Ù", "Ø£ÙˆÙ‚Ù", "Ø£ØºÙ„Ù‚", "Ø¥Ø·ÙØ§Ø¡", "ÙˆÙ‚Ù Ø§Ù„ØªØ´ØºÙŠÙ„", "Ø§Ø±ÙØ¹", "Ø®ÙØ¶",
#     "Ø²ÙˆØ¯", "Ù‚Ù„Ù„", "Ø²ÙŠØ¯", "Ù†Ù‚Øµ", "ØºÙŠØ±", "Ø§Ø¶Ø¨Ø·", "Ø¹Ø¯Ù„", "Ø¨Ø¯Ù„", "Ù‚Ù… Ø¨Ø¶Ø¨Ø·",
#     "Ù‚Ù… Ø¨ØªØ¹Ø¯ÙŠÙ„", "Ø®ÙÙ", "Ø³Ø·Ø¹", "Ø§Ø¬Ø¹Ù„ Ø§Ù„Ø¥Ø¶Ø§Ø¡Ø© Ø£Ù‚ÙˆÙ‰", "Ø§Ø¬Ø¹Ù„ Ø§Ù„Ø¥Ø¶Ø§Ø¡Ø© Ø£Ø¶Ø¹Ù",
#     "ØºÙŠØ± Ø§Ù„Ù„ÙˆÙ† Ø¥Ù„Ù‰", "Ø§Ø¶Ø¨Ø· Ø§Ù„Ø³Ø·ÙˆØ¹ Ø¥Ù„Ù‰", "Ø§Ø¬Ø¹Ù„ Ø§Ù„Ø³Ø·ÙˆØ¹", "Ø§Ø¶Ø¨Ø· Ø³Ø±Ø¹Ø© Ø§Ù„Ù…Ø±ÙˆØ­Ø© Ø¥Ù„Ù‰",
#     "Ø§Ø¨Ø¯Ø£", "Ø£ÙˆÙ‚Ù", "Ø§Ø³ØªØ£Ù†Ù", "Ø§Ø³ØªÙ…Ø±Ø§Ø±", "ÙˆÙ‚Ù Ù…Ø¤Ù‚Øª", "Ø§Ø¶Ø¨Ø· Ù…Ø¤Ù‚Øª", "Ø­Ø¯Ø¯ ÙˆÙ‚Øª Ø§Ù„ØªØ´ØºÙŠÙ„",
#     "Ø´ØºÙ„ Ø¹Ù†Ø¯", "Ø£Ø·ÙØ¦ Ø¹Ù†Ø¯", "Ù‚Ù… Ø¨ØªÙ…ÙƒÙŠÙ†", "Ø¹Ø·Ù„", "Ø§Ø±Ø¨Ø·", "ÙˆØµÙ„", "Ø§Ø¹Ø±Ø¶ Ø§Ù„Ø­Ø§Ù„Ø©",
#     "ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø§Ù„Ø©", "Ù‡Ù„ Ù‡Ùˆ ÙŠØ¹Ù…Ù„", "Ù‡Ù„ Ù‡Ùˆ Ù…ØºÙ„Ù‚", "ÙˆØ¶Ø¹ Ø§Ù„Ù†ÙˆÙ…", "ÙˆØ¶Ø¹ Ø§Ù„Ø³ÙŠÙ†Ù…Ø§",
#     "ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ±", "ØªØµØ¨Ø­ Ø¹Ù„Ù‰ Ø®ÙŠØ±"
# ]

# devices_list_ar = [
#     "Ø§Ù„Ø£Ø¶ÙˆØ§Ø¡", "Ø§Ù„Ø¶ÙˆØ¡", "Ø§Ù„Ù…ØµØ§Ø¨ÙŠØ­", "Ø§Ù„Ù…ØµØ¨Ø§Ø­", "Ø§Ù„Ù„Ù…Ø¨Ø§Øª", "Ø§Ù„Ù„Ù…Ø¨Ø©", "Ø§Ù„Ù„ÙŠØ¯Ø§Øª", "Ù†ÙˆØ±",
#     "Ø§Ù„Ù†ÙˆØ±", "Ø§Ù„Ø¥Ø¶Ø§Ø¡Ø©", "Ø§Ù„Ù„Ù…Ø¨Ø© Ø§Ù„Ø°ÙƒÙŠØ©", "Ù…ØµØ¨Ø§Ø­ Ø§Ù„Ø³Ù‚Ù", "Ø¥Ø¶Ø§Ø¡Ø© Ø§Ù„Ø´Ø±ÙŠØ·", "Ø§Ù„Ù…Ø±ÙˆØ­Ø©",
#     "Ø§Ù„Ù…Ø±Ø§ÙˆØ­", "Ù…Ø±ÙˆØ­Ø© Ø§Ù„Ø³Ù‚Ù", "Ø§Ù„Ù…Ø±ÙˆØ­Ø© Ø§Ù„Ø°ÙƒÙŠØ©", "Ø´ÙØ§Ø· Ø§Ù„Ù‡ÙˆØ§Ø¡", "Ù…Ø±ÙˆØ­Ø© Ø§Ù„Ø·Ø§ÙˆÙ„Ø©", "Ø§Ù„Ø¨Ø§Ø¨",
#     "Ø§Ù„Ø£Ø¨ÙˆØ§Ø¨", "Ø§Ù„Ø¨Ø§Ø¨ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ", "Ø¨Ø§Ø¨ Ø§Ù„Ù…Ø¯Ø®Ù„", "Ø§Ù„Ø¨Ø§Ø¨ Ø§Ù„Ø£Ù…Ø§Ù…ÙŠ", "Ø§Ù„Ø¨Ø§Ø¨ Ø§Ù„Ø®Ù„ÙÙŠ", "Ø¨Ø§Ø¨ Ø§Ù„Ø¬Ø±Ø§Ø¬",
#     "Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§", "Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª", "ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©", "ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ø£Ù…Ù†", "ÙƒØ§Ù…ÙŠØ±Ø§ CCTV", "Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©",
#     "Ø§Ù„Ø³ØªØ§Ø¦Ø±", "Ø§Ù„Ø³ØªØ§Ø±Ø©", "Ø§Ù„Ø¨Ø±Ø§Ø¯ÙŠ", "Ø§Ù„Ø´ÙŠØ´", "Ø§Ù„Ø³ØªØ§Ø¦Ø± Ø§Ù„Ø°ÙƒÙŠØ©", "Ø§Ù„ØºØ§Ù„Ù‚", "Ù…Ø¸Ù„Ø© Ø§Ù„Ù†Ø§ÙØ°Ø©"
# ]

# locations_list_ar = [
#     "ØºØ±ÙØ© Ø§Ù„Ù…Ø¹ÙŠØ´Ø©", "Ø§Ù„ØµØ§Ù„Ø©", "Ø§Ù„ØµØ§Ù„ÙˆÙ†", "Ø§Ù„Ø±ÙŠØ³ÙŠØ¨Ø´Ù†", "Ø§Ù„Ø±ÙŠØ³Ø¨Ø´Ù†", "Ø§Ù„ØºØ±ÙØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©",
#     "ØºØ±ÙØ© Ø§Ù„Ù†ÙˆÙ…", "ØºØ±ÙØ© Ø§Ù„Ù†ÙˆÙ… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©", "ØºØ±ÙØ© Ø§Ù„Ø¶ÙŠÙˆÙ", "ØºØ±ÙØªÙŠ", "Ø­Ø¬Ø±ØªÙŠ", "Ø§Ù„Ù…Ø·Ø¨Ø®",
#     "Ø§Ù„Ù…Ø·Ø¨Ø® Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ", "Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø·Ù‡ÙŠ", "Ù…ÙƒØ§Ù† Ø§Ù„Ø£ÙƒÙ„", "ØºØ±ÙØ© Ø§Ù„Ø·Ø¹Ø§Ù…", "Ø§Ù„Ø­Ù…Ø§Ù…", "Ø¯ÙˆØ±Ø© Ø§Ù„Ù…ÙŠØ§Ù‡",
#     "Ø§Ù„ØªÙˆØ§Ù„ÙŠØª", "Ø§Ù„Ù…Ø±Ø­Ø§Ø¶", "Ø§Ù„Ø­Ù…Ø§Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ", "Ø§Ù„Ø´Ø±ÙØ©", "Ø§Ù„Ø¨Ù„ÙƒÙˆÙ†Ø©", "Ø§Ù„ØªØ±Ø§Ø³", "Ø§Ù„ÙÙ†Ø§Ø¡",
#     "Ø§Ù„Ø¨Ø§Ø­Ø©", "Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©", "Ø§Ù„ÙƒØ±Ø§Ø¬", "Ø§Ù„Ø¬Ø±Ø§Ø¬", "Ù…ÙƒØ§Ù† Ø§Ù„Ø³ÙŠØ§Ø±Ø©", "Ø§Ù„Ù…ÙˆÙ‚Ù"
# ]

# nltk.download('punkt', quiet=True)

# # Load Whisper
# print("ğŸ”„ Loading Whisper model...")
# model = whisper.load_model("medium")
# print("âœ… Whisper model loaded.")

# # Arabic NER with MARBERT
# bert_model = "UBC-NLP/MARBERTv2"
# tokenizer = AutoTokenizer.from_pretrained(bert_model)
# model_bert = AutoModelForTokenClassification.from_pretrained(bert_model)
# ner_pipeline = pipeline("ner", model=model_bert, tokenizer=tokenizer)

# spell_en = Speller(lang='en')
# spell_checker_ar = SpellChecker(language='ar')

# ACTION_KEYWORDS = {
#     "open": {
#         "open", "unlock", "activate", "enable", "turn on", "switch on", "start",
#         "schedule", "set timer for", "turn on at", "sync", "connect", "good morning",
#         "movie mode", "night mode", "Ø´ØºÙ„", "Ø£Ø´ØºÙ„", "Ø§ÙØªØ­", "Ø£ÙØªØ­", "Ø´ØºÙ‘Ù„", "Ù‚Ù… Ø¨ØªØ´ØºÙŠÙ„",
#         "Ù‚Ù… Ø¨Ø¥Ø¶Ø§Ø¡Ø©", "Ø§Ø¨Ø¯Ø£", "Ø§Ø³ØªØ£Ù†Ù", "Ø§Ø³ØªÙ…Ø±Ø§Ø±", "Ø­Ø¯Ø¯ ÙˆÙ‚Øª Ø§Ù„ØªØ´ØºÙŠÙ„", "Ø´ØºÙ„ Ø¹Ù†Ø¯",
#         "Ù‚Ù… Ø¨ØªÙ…ÙƒÙŠÙ†", "Ø§Ø±Ø¨Ø·", "ÙˆØµÙ„", "ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ±", "ÙˆØ¶Ø¹ Ø§Ù„Ø³ÙŠÙ†Ù…Ø§", "ÙˆØ¶Ø¹ Ø§Ù„Ù†ÙˆÙ…"
#     },
#     "close": {
#         "close", "lock", "deactivate", "disable", "turn off", "switch off", "stop",
#         "pause", "turn off at", "Ø£Ø·ÙØ¦", "Ø§ØºÙ„Ù‚", "Ø¥ÙŠÙ‚Ø§Ù", "Ø£ÙˆÙ‚Ù", "Ø£ØºÙ„Ù‚", "Ø¥Ø·ÙØ§Ø¡",
#         "ÙˆÙ‚Ù Ø§Ù„ØªØ´ØºÙŠÙ„", "ÙˆÙ‚Ù Ù…Ø¤Ù‚Øª", "Ø£ÙˆÙ‚Ù", "Ø£Ø·ÙØ¦ Ø¹Ù†Ø¯", "Ø¹Ø·Ù„", "ØªØµØ¨Ø­ Ø¹Ù„Ù‰ Ø®ÙŠØ±"
#     },
#     "increase": {
#         "increase", "raise", "brighten", "make brighter", "set brightness to",
#         "set fan speed to", "Ø²ÙˆØ¯", "Ø§Ø±ÙØ¹", "Ø²ÙŠØ¯", "Ø§Ø¶Ø¨Ø· Ø§Ù„Ø³Ø·ÙˆØ¹ Ø¥Ù„Ù‰", "Ø§Ø¬Ø¹Ù„ Ø§Ù„Ø³Ø·ÙˆØ¹",
#         "Ø³Ø·Ø¹", "Ø§Ø¬Ø¹Ù„ Ø§Ù„Ø¥Ø¶Ø§Ø¡Ø© Ø£Ù‚ÙˆÙ‰", "Ø§Ø¶Ø¨Ø· Ø³Ø±Ø¹Ø© Ø§Ù„Ù…Ø±ÙˆØ­Ø© Ø¥Ù„Ù‰"
#     },
#     "decrease": {
#         "decrease", "lower", "dim", "make dimmer", "Ù‚Ù„Ù„", "Ù†Ù‚Øµ", "Ø®ÙØ¶", "Ø§Ø®ÙØ¶",
#         "Ø®ÙÙ", "Ø§Ø¬Ø¹Ù„ Ø§Ù„Ø¥Ø¶Ø§Ø¡Ø© Ø£Ø¶Ø¹Ù"
#     },
# }

# DEVICE_KEYWORDS = {
#     "light": {
#         "light", "lights", "lamp", "bulb", "leds", "ceiling light", "strip lights",
#         "Ø§Ù„Ø£Ø¶ÙˆØ§Ø¡", "Ø§Ù„Ø¶ÙˆØ¡", "Ø§Ù„Ù…ØµØ§Ø¨ÙŠØ­", "Ø§Ù„Ù…ØµØ¨Ø§Ø­", "Ø§Ù„Ù„Ù…Ø¨Ø§Øª", "Ø§Ù„Ù„Ù…Ø¨Ø©", "Ø§Ù„Ù„ÙŠØ¯Ø§Øª",
#         "Ù†ÙˆØ±", "Ø§Ù„Ù†ÙˆØ±", "Ø§Ù„Ø¥Ø¶Ø§Ø¡Ø©", "Ø§Ù„Ù„Ù…Ø¨Ø© Ø§Ù„Ø°ÙƒÙŠØ©", "Ù…ØµØ¨Ø§Ø­ Ø§Ù„Ø³Ù‚Ù", "Ø¥Ø¶Ø§Ø¡Ø© Ø§Ù„Ø´Ø±ÙŠØ·"
#     },
#     "fan": {
#         "fan", "ceiling fan", "exhaust fan", "desk fan", "Ø§Ù„Ù…Ø±ÙˆØ­Ø©", "Ø§Ù„Ù…Ø±Ø§ÙˆØ­", "Ù…Ø±ÙˆØ­Ø© Ø§Ù„Ø³Ù‚Ù",
#         "Ø§Ù„Ù…Ø±ÙˆØ­Ø© Ø§Ù„Ø°ÙƒÙŠØ©", "Ø´ÙØ§Ø· Ø§Ù„Ù‡ÙˆØ§Ø¡", "Ù…Ø±ÙˆØ­Ø© Ø§Ù„Ø·Ø§ÙˆÙ„Ø©"
#     },
#     "door": {
#         "door", "front door", "main door", "back door", "garage door", "Ø§Ù„Ø¨Ø§Ø¨", "Ø§Ù„Ø£Ø¨ÙˆØ§Ø¨",
#         "Ø§Ù„Ø¨Ø§Ø¨ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ", "Ø¨Ø§Ø¨ Ø§Ù„Ù…Ø¯Ø®Ù„", "Ø§Ù„Ø¨Ø§Ø¨ Ø§Ù„Ø£Ù…Ø§Ù…ÙŠ", "Ø§Ù„Ø¨Ø§Ø¨ Ø§Ù„Ø®Ù„ÙÙŠ", "Ø¨Ø§Ø¨ Ø§Ù„Ø¬Ø±Ø§Ø¬"
#     },
#     "curtain": {
#         "curtain", "curtains", "blinds", "shades", "window covers", "Ø§Ù„Ø³ØªØ§Ø±Ø©", "Ø§Ù„Ø³ØªØ§Ø¦Ø±",
#         "Ø§Ù„Ø¨Ø±Ø§Ø¯ÙŠ", "Ø§Ù„Ø´ÙŠØ´", "Ø§Ù„Ø³ØªØ§Ø¦Ø± Ø§Ù„Ø°ÙƒÙŠØ©", "Ø§Ù„ØºØ§Ù„Ù‚", "Ù…Ø¸Ù„Ø© Ø§Ù„Ù†Ø§ÙØ°Ø©"
#     },
#     "camera": {
#         "camera", "security camera", "cctv", "surveillance", "Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§", "Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª",
#         "ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©", "ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ø£Ù…Ù†", "ÙƒØ§Ù…ÙŠØ±Ø§ cctv", "Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©"
#     }
# }

# LOCATION_KEYWORDS = {
#     "kitchen": {
#         "kitchen", "cooking area", "Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø·Ù‡ÙŠ", "Ø§Ù„Ù…Ø·Ø¨Ø®", "Ø§Ù„Ù…Ø·Ø¨Ø® Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"
#     },
#     "bathroom": {
#         "bathroom", "restroom", "toilet", "washroom", "Ø§Ù„Ø­Ù…Ø§Ù…", "Ø¯ÙˆØ±Ø© Ø§Ù„Ù…ÙŠØ§Ù‡",
#         "Ø§Ù„ØªÙˆØ§Ù„ÙŠØª", "Ø§Ù„Ù…Ø±Ø­Ø§Ø¶", "Ø§Ù„Ø­Ù…Ø§Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"
#     },
#     "room": {
#         "room", "my room", "ØºØ±ÙØ©", "ØºØ±ÙÙ‡", "ØºØ±ÙØªÙŠ", "Ø­Ø¬Ø±ØªÙŠ", "guest room",
#         "ØºØ±ÙØ© Ø§Ù„Ø¶ÙŠÙˆÙ", "bedroom", "ØºØ±ÙØ© Ø§Ù„Ù†ÙˆÙ…", "master bedroom", "ØºØ±ÙØ© Ø§Ù„Ù†ÙˆÙ… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"
#     },
#     "living room": {
#         "living room", "hall", "lounge", "main room", "reception", "ØµØ§Ù„Ø©", "Ø§Ù„ØµØ§Ù„ÙˆÙ†",
#         "Ø§Ù„Ø±ÙŠØ³ÙŠØ¨Ø´Ù†", "Ø§Ù„Ø±ÙŠØ³Ø¨Ø´Ù†", "Ø§Ù„ØºØ±ÙØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"
#     },
#     "outdoor": {
#         "balcony", "Ø§Ù„Ø´Ø±ÙØ©", "Ø§Ù„Ø¨Ù„ÙƒÙˆÙ†Ø©", "terrace", "Ø§Ù„ØªØ±Ø§Ø³", "patio", "Ø§Ù„ÙÙ†Ø§Ø¡",
#         "Ø§Ù„Ø¨Ø§Ø­Ø©", "porch", "Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©"
#     },
#     "garage": {
#         "garage", "carport", "Ø§Ù„ÙƒØ±Ø§Ø¬", "Ø§Ù„Ø¬Ø±Ø§Ø¬", "Ù…ÙƒØ§Ù† Ø§Ù„Ø³ÙŠØ§Ø±Ø©", "Ø§Ù„Ù…ÙˆÙ‚Ù"
#     }
# }


# # ====== Normalization Helpers ======
# def match_all_from_dict(text: str, keyword_dict: dict) -> list[str]:
#     matches = []
#     for key, variations in keyword_dict.items():
#         for v in variations:
#             if v in text:
#                 matches.append(key)
#                 break  # Avoid duplicates if multiple synonyms match
#     return matches

# # ====== Command Processors ======
# def process_english_command(command: str) -> tuple[list[str], list[str], list[str]]:
#     command = re.sub(r'[^\w\s]', '', command.lower())
#     print("process_english_command is : " , command)
#     actions = match_all_from_dict(command, ACTION_KEYWORDS)
#     devices = match_all_from_dict(command, DEVICE_KEYWORDS)
#     locations = match_all_from_dict(command, LOCATION_KEYWORDS)
#     print("actions is : " , actions ,"devices is : " , devices ,"locations is : " , locations )
#     return actions, devices, locations

# def process_arabic_command(command: str) -> tuple[list[str], list[str], list[str]]:
#     command = re.sub(r'[^\w\s\u0600-\u06FF]', '', command.lower())

#     actions = match_all_from_dict(command, ACTION_KEYWORDS)
#     devices = match_all_from_dict(command, DEVICE_KEYWORDS)
#     locations = match_all_from_dict(command, LOCATION_KEYWORDS)

#     return actions, devices, locations


# # Record mic input
# def record_audio(duration=15, filename='mic_input.wav', samplerate=16000):
#     print("ğŸ™ï¸ Recording from microphone...")
#     recording = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=1, dtype='int16')
#     sd.wait()
#     with wave.open(filename, 'w') as wf:
#         wf.setnchannels(1)
#         wf.setsampwidth(2)
#         wf.setframerate(samplerate)
#         wf.writeframes(recording.tobytes())
#     print(f"âœ… Recording saved to: {filename}")
#     return filename

# # NLP Processing
# # âœ… Process English Commands (Updated)

# # Arabic pipeline
# def process_audio_en(file_path):
#     print(f"ğŸ” Processing file: {file_path}")

#     if not os.path.exists(file_path):
#         print("âŒ Error: File not found!")
#         return {'error': 'File not found'}

#     try:
#         # âœ… Transcribe audio using Whisper
#         result = model.transcribe(file_path, language="en")
#         transcribed_text = result.get('text', '').strip()

#         if not transcribed_text:
#             print("âš  Warning: Whisper did not return any text!")
#             return {'error': 'No speech detected'}

#         print(f"ğŸ“ Transcribed Text: {transcribed_text}")

#         # âœ… Split the transcribed text into multiple commands (assuming ',' or 'and' separate commands)
#         commands = transcribed_text.lower().replace(" and ", ",").split(",")

#         sent_commands = []

#         for command in commands:
#             command = command.strip()
#             tokens = word_tokenize(command)
#             corrected_tokens = [spell_en(t) for t in tokens]
#             print("the send to process_english_command : ",corrected_tokens)
#             print("the type is : ",type(corrected_tokens))
#             actions, rooms ,devices  = process_english_command(" ".join(corrected_tokens))

#             if not actions or not devices:
#                 print(f"âš  Warning: Invalid command detected - {command}")
#                 continue  # Skip invalid commands

#             location = rooms[0] if rooms else ""
#             command_to_esp = f"{actions[0]} {devices[0]} {location}".strip()
#             print(f"ğŸš€ Sending to ESP32: {command_to_esp}")

#         return {
#             'text': transcribed_text,
#             'sent_commands': sent_commands,
#         }

#     except Exception as e:
#         return {'error': str(e)}

# # Arabic NER with MBERT
# bert_model = "bert-base-multilingual-cased"
# tokenizer = AutoTokenizer.from_pretrained(bert_model)
# model_bert = AutoModelForTokenClassification.from_pretrained(bert_model)
# ner_pipeline = pipeline("ner", model=model_bert, tokenizer=tokenizer)

# def process_audio_ar(file_path):
#     print(f"ğŸ” Processing file: {file_path}")

#     if not os.path.exists(file_path):
#         print("âŒ Error: File not found!")
#         return {'error': 'File not found'}

#     try:
#         # âœ… Transcribe audio using Whisper (Dummy transcription)
#         result = model.transcribe(file_path, language="ar")
#         transcribed_text = result.get('text', '').strip()

#         if not transcribed_text:
#             print("âš  Warning: Whisper did not return any text!")
#             return {'error': 'No speech detected'}

#         print(f"ğŸ“ Transcribed Text: {transcribed_text}")

#         # mBERT Named Entity Recognition
#         ner_results = ner_pipeline(transcribed_text)
#         print("ğŸ” mBERT NER Results:", ner_results)

#         # Extract tokens and labels
#         entities = [entity['word'] for entity in ner_results if entity['score'] > 0.95]
#         print(f"âœ¨ Detected Entities: {entities}")

#         tokens = word_tokenize(transcribed_text)
#         corrected_tokens = [spell_checker_ar.correction(t) for t in tokens]

#         actions, rooms, devices = process_arabic_command(transcribed_text)  # Use original Arabic text without translation

#         # âœ… Print extracted command
#         print(f"ğŸ”¹ Actions: {actions}")
#         print(f"ğŸ”¹ Rooms: {rooms}")
#         print(f"ğŸ”¹ Devices: {devices}")

#         return {
#             'text': transcribed_text,
#             'entities': entities,
#             'corrected_tokens': corrected_tokens,
#             'actions': actions,
#             'rooms': rooms,
#             'devices': devices
#         }
#     except Exception as e:
#         print(f"âŒ Error: {e}")
#         return {'error': str(e)}

# # Main loop
# if __name__ == "__main__":
#     lang_choice = input("ğŸŒ Choose language (en/ar): ").strip().lower()

#     audio_path = record_audio()

#     if lang_choice == "ar":
#         process_audio_ar(audio_path)
#     else:
#         process_audio_en(audio_path)

#     os.remove(audio_path)

# from langdetect import detect, DetectorFactory
# DetectorFactory.seed = 0  # Ù„Ø¶Ù…Ø§Ù† Ù†ÙØ³ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ ÙƒÙ„ Ù…Ø±Ø©


# text = input("Write the sentence that you want to know its language: ")


# language = detect(text)

# print(f"Expected language: {language}")

# import fasttext

# # Load pre-trained language identification model
# model = fasttext.load_model("lid.176.bin")

# # Input from user
# text = input("Enter a sentence to detect its language: ")

# # Predict the language
# prediction = model.predict(text)
# language_label = prediction[0][0].replace("__label__", "")
# # confidence = prediction[1][0]

# # Output result
# print(f"Detected language: {language_label}")
# # print(f"Confidence: {confidence:.2f}")

# import fasttext
# import tkinter as tk
# from tkinter import messagebox

# # Load the model
# model = fasttext.load_model("lid.176.bin")

# # Function to detect language
# def detect_language():
#     text = input_field.get()
#     if not text.strip():
#         messagebox.showwarning("Input Error", "Please enter some text.")
#         return
#     prediction = model.predict(text)
#     language_label = prediction[0][0].replace("__label__", "")
#     # confidence = prediction[1][0]
#     result_label.config(text=f"Detected language: {language_label}")
#     # result_label.config(text=f"Detected language: {language_label} (Confidence: {confidence:.2f})")

# # Create GUI window-
# window = tk.Tk()
# window.title("Language Detection (fastText)")
# window.geometry("400x200")

# # Input field
# tk.Label(window, text="Enter a sentence:").pack(pady=5)
# input_field = tk.Entry(window, width=50)
# input_field.pack(pady=5)

# # Detect button
# detect_btn = tk.Button(window, text="Detect Language", command=detect_language)
# detect_btn.pack(pady=10)

# # Result label
# result_label = tk.Label(window, text="", fg="blue", font=("Arial", 12))
# result_label.pack(pady=10)

# # Run the GUI
# window.mainloop()






import os
import re

from audio_utils import record_audio
from whisper_utils import transcribe_audio
from nlp_en import process_english_command
from nlp_ar import process_arabic_command


def split_commands(text: str, lang: str) -> list[str]:
    t = text.strip()
    t = t.replace("ØŒ", " ")
    if lang == "ar":
        parts = re.split(r"\bÙˆ\b|\bØ«Ù…\b|\bÙˆØ¨Ø¹Ø¯ÙŠÙ†\b", t)
    else:
        t = t.lower().replace(" and ", ",")
        parts = [p for p in t.split(",")]
    return [p.strip() for p in parts if p.strip()]


if __name__ == "__main__":
    lang_choice = input("ğŸŒ Choose language (en/ar): ").strip().lower()

    audio_path = record_audio(duration=10)

    if lang_choice == "ar":
        text = transcribe_audio(audio_path, "ar")
        print("ğŸ“ Text:", text)

        commands = split_commands(text, "ar")
        for i, cmd in enumerate(commands, 1):
            actions, devices, locations = process_arabic_command(cmd)
            print(f"\nâ€” Command #{i}: {cmd}")
            print("âš™ï¸ Actions:", actions)
            print("ğŸ“Ÿ Devices:", devices)
            print("ğŸ“ Locations:", locations)

    else:
        text = transcribe_audio(audio_path, "en")
        print("ğŸ“ Text:", text)

        commands = split_commands(text, "en")
        for i, cmd in enumerate(commands, 1):
            actions, devices, locations = process_english_command(cmd)
            print(f"\nâ€” Command #{i}: {cmd}")
            print("âš™ï¸ Actions:", actions)
            print("ğŸ“Ÿ Devices:", devices)
            print("ğŸ“ Locations:", locations)

    try:
        os.remove(audio_path)
    except OSError:
        pass
